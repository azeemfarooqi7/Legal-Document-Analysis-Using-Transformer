{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "def extract_zip(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "def parse_xml_file(filepath):\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "    except UnicodeDecodeError:\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='latin-1') as file:\n",
        "                content = file.read()\n",
        "        except UnicodeDecodeError:\n",
        "            with open(filepath, 'r', encoding='iso-8859-1', errors='ignore') as file:\n",
        "                content = file.read()\n",
        "\n",
        "    soup = BeautifulSoup(content, 'xml')\n",
        "\n",
        "    case_data = {}\n",
        "    case_data['name'] = soup.find('name').text if soup.find('name') else None\n",
        "    case_data['AustLII'] = soup.find('AustLII').text if soup.find('AustLII') else None\n",
        "\n",
        "    catchphrases = soup.find_all('catchphrase')\n",
        "    case_data['catchphrases'] = [catchphrase.text for catchphrase in catchphrases]\n",
        "\n",
        "    sentences = soup.find_all('sentence')\n",
        "    case_data['sentences'] = [sentence.text for sentence in sentences]\n",
        "\n",
        "    return case_data\n",
        "\n",
        "def parse_all_xml_files(directory):\n",
        "    all_cases = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.xml'):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            case_data = parse_xml_file(filepath)\n",
        "            all_cases.append(case_data)\n",
        "\n",
        "    return all_cases\n",
        "\n",
        "# Specify the path to the zip file and the directory to extract to\n",
        "zip_path = '/content/drive/MyDrive/Legal Analysis.zip'\n",
        "extract_to = '/content/legal_analysis'\n",
        "\n",
        "# Extract the zip file\n",
        "extract_zip(zip_path, extract_to)\n",
        "\n",
        "# Parse all XML files from the extracted directory\n",
        "all_cases = parse_all_xml_files(extract_to)\n",
        "\n",
        "# Convert the list of dictionaries to a pandas DataFrame for easier analysis\n",
        "df = pd.DataFrame(all_cases)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKwDvmth_o2s",
        "outputId": "502c3084-401f-4e5f-c94e-c41d528bf392"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   name  \\\n",
            "0     Gordon v Commonwealth of Australia [2008] FCA ...   \n",
            "1     Mahmoud v The Owners* Corporation Strata Plan ...   \n",
            "2     MZPAO v Minister for Immigration and Citizensh...   \n",
            "3     Citrus Queensland Pty Ltd v Sunstate Orchards ...   \n",
            "4     Secretary, Department of Employment and Workpl...   \n",
            "...                                                 ...   \n",
            "4187  Walters v Commissioner of Taxation (No. 2) [20...   \n",
            "4188  Different Solutions Pty Limited v Commissioner...   \n",
            "4189  Yalniz v Minister for Immigration and Citizens...   \n",
            "4190  Bradken Resources Pty Ltd v Lynx Engineering C...   \n",
            "4191  University of Western Australia v Gray (No 4) ...   \n",
            "\n",
            "                                                AustLII  \\\n",
            "0     http://www.austlii.edu.au/au/cases/cth/FCA/200...   \n",
            "1     http://www.austlii.edu.au/au/cases/cth/FCA/200...   \n",
            "2     http://www.austlii.edu.au/au/cases/cth/FCA/200...   \n",
            "3     http://www.austlii.edu.au/au/cases/cth/FCA/200...   \n",
            "4     http://www.austlii.edu.au/au/cases/cth/FCA/200...   \n",
            "...                                                 ...   \n",
            "4187  http://www.austlii.edu.au/au/cases/cth/FCA/200...   \n",
            "4188  http://www.austlii.edu.au/au/cases/cth/FCA/200...   \n",
            "4189  http://www.austlii.edu.au/au/cases/cth/FCA/200...   \n",
            "4190  http://www.austlii.edu.au/au/cases/cth/FCA/200...   \n",
            "4191  http://www.austlii.edu.au/au/cases/cth/FCA/200...   \n",
            "\n",
            "                                           catchphrases  \\\n",
            "0     [\"id=c0\">disability discrimination, \"id=c1\">ap...   \n",
            "1     [\"id=c0\">application for annulment of sequestr...   \n",
            "2     [\"id=c0\">appeal from decision of federal magis...   \n",
            "3     [\"id=c0\">pleadings, \"id=c1\">amendment, \"id=c2\"...   \n",
            "4     [\"id=c0\">social welfare and services, \"id=c1\">...   \n",
            "...                                                 ...   \n",
            "4187  [\"id=c0\">resolution of the question of costs a...   \n",
            "4188  [\"id=c0\">search warrants, \"id=c1\">judicial rev...   \n",
            "4189  [\"id=c0\">cancellation of visa on character gro...   \n",
            "4190  [\"id=c0\">patents, \"id=c1\">unjustified threats,...   \n",
            "4191  [\"id=c0\">appointment of receiver under federal...   \n",
            "\n",
            "                                              sentences  \n",
            "0     [\\nCONTENTS\\n \\n \\n Mr Gordon's career \\n \\n [...  \n",
            "1     [Mahmoud v The Owners* Corporation Strata Plan...  \n",
            "2     [\\n BACKGROUND \\n \\n1 This is an appeal agains...  \n",
            "3     [\\n \\n1 This morning in Court the applicants s...  \n",
            "4     [\\n Background \\n \\n1 Mrs Dolores Vanderpluym ...  \n",
            "...                                                 ...  \n",
            "4187  [\\n \\n1 On 20 August 2007, the Court delivered...  \n",
            "4188  [\\n Background \\n \\n1 The hearing of this matt...  \n",
            "4189  [\\n \\n The proceeding \\n \\n1 Mr Selahattin Yal...  \n",
            "4190  [\\n \\n1 The Court has three applications befor...  \n",
            "4191  [\\n \\n 1 The third respondent, the Cancer Rese...  \n",
            "\n",
            "[4192 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Path to the ZIP file and extraction\n",
        "zip_file_path = '/content/drive/MyDrive/Legal Analysis.zip'\n",
        "extract_to = '/content/legal_analysis'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "# Function to parse XML files\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def parse_xml_file(filepath):\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "    except UnicodeDecodeError:\n",
        "        with open(filepath, 'r', encoding='ISO-8859-1') as file:\n",
        "            content = file.read()\n",
        "\n",
        "    soup = BeautifulSoup(content, 'xml')\n",
        "\n",
        "    case_data = {}\n",
        "    case_data['name'] = soup.find('name').text if soup.find('name') else None\n",
        "    case_data['AustLII'] = soup.find('AustLII').text if soup.find('AustLII') else None\n",
        "\n",
        "    catchphrases = soup.find_all('catchphrase')\n",
        "    case_data['catchphrases'] = [catchphrase.text for catchphrase in catchphrases]\n",
        "\n",
        "    sentences = soup.find_all('sentence')\n",
        "    case_data['sentences'] = [sentence.text for sentence in sentences]\n",
        "\n",
        "    return case_data\n",
        "\n",
        "def parse_all_xml_files(directory):\n",
        "    all_cases = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.xml'):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            case_data = parse_xml_file(filepath)\n",
        "            all_cases.append(case_data)\n",
        "\n",
        "    return all_cases\n",
        "\n",
        "# Parse all XML files from the extracted directory\n",
        "all_cases = parse_all_xml_files(extract_to)\n",
        "\n",
        "# Convert the list of dictionaries to a pandas DataFrame for easier analysis\n",
        "df = pd.DataFrame(all_cases)\n",
        "\n",
        "# Preprocess Data\n",
        "df['label'] = df['catchphrases'].apply(lambda x: len(x) % 2)  # Simplified binary labels for demonstration\n",
        "df['text'] = df['sentences'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Reduce the dataset size for faster training\n",
        "df = df.sample(n=500, random_state=42)\n",
        "\n",
        "# Split data into training and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class LegalDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = LegalDataset(train_texts.tolist(), train_labels.tolist(), tokenizer, max_len=128)\n",
        "test_dataset = LegalDataset(test_texts.tolist(), test_labels.tolist(), tokenizer, max_len=128)\n",
        "\n",
        "# Define and train the model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Binary classification\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,  # Reduced for faster training\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=10,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds, labels = p\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    prec = precision_score(labels, preds, average='weighted')\n",
        "    rec = recall_score(labels, preds, average='weighted')\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate model\n",
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "SYPoprNzBEDC",
        "outputId": "2e240314-a6f1-4685-aabf-e009b128011f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 7/25 02:48 < 10:05, 0.03 it/s, Epoch 0.24/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 11:31, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.723600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.693100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:54]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.6599414348602295, 'eval_accuracy': 0.58, 'eval_precision': 0.6157388316151202, 'eval_recall': 0.58, 'eval_f1': 0.45193675889328055, 'eval_runtime': 65.453, 'eval_samples_per_second': 1.528, 'eval_steps_per_second': 0.107, 'epoch': 1.0}\n"
          ]
        }
      ]
    }
  ]
}